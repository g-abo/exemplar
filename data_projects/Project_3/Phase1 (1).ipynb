{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery of Higgs Boson\n",
    "In this assignment, we'll use data directly from the LHC (Large Hadron Collider) along with Monte-Carlo (MC) simulations to claim the discovery of the Higgs Boson! Please make sure to use the seed 6372 for both PyTorch and Numpy.\n",
    "\n",
    "<b> Due Date: 05/03/2024 11:59pm</b> <br>\n",
    "<b> Submission: Output this file as both .ipynb and .pdf files and zip them before uploading to Gradescope</b> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, kstest\n",
    "import util\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Important for re-production\n",
    "torch.manual_seed(6372)\n",
    "np.random.seed(6372)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setup\n",
    "\n",
    "At the LHC, protons are accelerated to incredible speeds before they are collided. The collision results in sub-atomic particles that go through multiple phases of decay. These final decay products are the observables by the LHC detectors. Thus, the Higgs Boson is not observed directly but indirectly through its decay products. Our job is to conclude the existence of the Higgs Boson by observing such decay products. However, things aren't that simple. There are two obstacles to overcome.\n",
    "\n",
    "First, data generated directly from LHC collisions is huge (on the order of billions of entries). Thus, there has to be some filtering. The first type of filtering is online filtering. This happens automatically by LHC detectors to discard any events/collisions that aren't of primary importance for study, which reduces the size of data by few orders of magnitude. After online filtering, there comes off-line filtering. This filtering happens through the use of sophisticated knowledge of theoritical physics and is typically conducted by humans. <b>The data we have for this assignment comes after these two stages of filtering and is only ~ 500 entries</b>.\n",
    "\n",
    "Second, even after such filtering, we have confounding decay processes that produce the same final decay products as the Higgs decay! In other words, it is impossible to conclude the Higgs Boson discovery by only looking at the <i>types</i> of decay particles. We then have to look at other properties like kinetic energy, momentum etc... However, such properties are continuous in nature and observing any one value doesn't say much. The only remaining solution, then, is to study the statistical properties of such observations.\n",
    "\n",
    "So how does statistics help us \"discover\" the Higgs Boson? Through hypothesis testing!\n",
    "\n",
    "Luckily, even with confounding processes producing identical types of decay particles, theoretical physics describes different statistical distributions for products coming from Higgs vs. products coming from background. We will denote distribution of data coming Higgs by $Q$ and distribution of data coming from background by $P_0$. To make things more complicated, there is more than one background process. So $P_0$ can only be discribed as a <i>mixture</i> of multiple background processes. In other words, \n",
    "$$\n",
    "P_0 = \\sum_i a_i P_{0, i}\n",
    "$$\n",
    "where $P_{0, i}$ is the background distribution generated from process $i$, and $a_i$ is the mixing ratio.\n",
    "\n",
    "Finally, $Q, P_{0, i}$ are not given to us explicitly. In other words, we don't have closed-form expressions for their CDF/PDF. We can only sample from $Q, P_{0, i}$ through Monte-Carlo simulations. We will use these Monte-Carlo samples to compare the real experimental data recorded at the LHC with what the data should be under the null hypothesis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>1. Data Loading</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1a) (0.5 pt) Read the experimental data from the LHC. Use <b>load_expr_data()</b> function provided in utils.py. Store the dataframe object in the variable <b>data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = util.load_expr_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1b) (0.5 pt) Print the size of the data and column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data:  (495, 38)\n",
      "Column names:  Index(['Unnamed: 0', 'Run', 'Event', 'PID1', 'Q1', 'E1', 'px1', 'py1', 'pz1',\n",
      "       'eta1', 'phi1', 'PID2', 'Q2', 'E2', 'px2', 'py2', 'pz2', 'eta2', 'phi2',\n",
      "       'PID3', 'Q3', 'E3', 'px3', 'py3', 'pz3', 'eta3', 'phi3', 'PID4', 'Q4',\n",
      "       'E4', 'px4', 'py4', 'pz4', 'eta4', 'phi4', 'mass', 'mass_z1',\n",
      "       'mass_z2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of the data: \", data.shape)\n",
    "print(\"Column names: \", data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1c) (0.5 pt) Read the simuated MC data. Use the <b>load_processes()</b> function provided in utils.py. Store the list of dataframe objects in a variable <b>mc_processes</b>. Note that the first element of the of the list is the MC Higgs process. The rest are MC background processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_processes = util.load_processes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1d) (0.5 pt) Print the number of MC background processes. Print the size of the Higgs MC process as well as each of the MC background processes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of MC background processes: 6\n",
      "size of the Higgs MC process: 37393\n",
      "MC background process size 1: 164425\n",
      "MC background process size 2: 106147\n",
      "MC background process size 3: 85903\n",
      "MC background process size 4: 4\n",
      "MC background process size 5: 8\n",
      "MC background process size 6: 2\n"
     ]
    }
   ],
   "source": [
    "num_background_processes = len(mc_processes) - 1\n",
    "higgs_size = len(mc_processes[0])\n",
    "\n",
    "print(\"number of MC background processes:\", num_background_processes)\n",
    "print(\"size of the Higgs MC process:\", higgs_size)\n",
    "\n",
    "background_sizes = [len(process) for process in mc_processes[1:]]\n",
    "for i, size in enumerate(background_sizes):\n",
    "    print(f\"MC background process size {i+1}:\", size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1e) (0.5 pt) Choose any MC process and print its column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names of the first MC process: Index(['Unnamed: 0', 'Run', 'Event', 'PID1', 'Q1', 'E1', 'px1', 'py1', 'pz1',\n",
      "       'eta1', 'phi1', 'PID2', 'Q2', 'E2', 'px2', 'py2', 'pz2', 'eta2', 'phi2',\n",
      "       'PID3', 'Q3', 'E3', 'px3', 'py3', 'pz3', 'eta3', 'phi3', 'PID4', 'Q4',\n",
      "       'E4', 'px4', 'py4', 'pz4', 'eta4', 'phi4', 'mass', 'mass_z1', 'mass_z2',\n",
      "       'signal'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Column names of the first MC process:\",mc_processes[1].columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1f) (0.5 pt) Compare MC column names with those of the experimental data. There should be one additional column for the MC processes. What is the meaning of this column? Why can't we have such column in the experimental data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extra 'signal' column is relating to the Higgs signal process, and it labels each simulated event as either being signal or background.\n",
    "Experimental data has a mixture of signal from the higgs and background, so due to unknowability of signal, we can\n",
    "merely hypothesize the source of any observed signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>2. Data Munging</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2a) (0.5 pt) Below we have defined two lists. <b>object_cols</b> specifies the categorial features of our data. In our case, it describes the <b>PID</b> (Particle ID). <b>irrelevant_cols</b> describes predictors to be discarded.\n",
    "\n",
    "You task is to one-hot encode the categorial features and discard the irrelevant features. Use <b>encoder, trim</b> from utils.py. Read the documentation carefully. It should be clear enough for you to use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_cols = ['PID1', 'PID2', 'PID3', 'PID4'] \n",
    "\n",
    "# Vanilla\n",
    "irrelevant_cols = ['Unnamed: 0', 'Run', 'Event', 'Q1', 'Q2', 'Q3', 'Q4', 'mass', 'mass_z1', 'mass_z2']\n",
    "\n",
    "# your code here\n",
    "OH_encoder = util.encoder(mc_processes, object_cols)\n",
    "data, mc_processes = util.trim(OH_encoder, object_cols, irrelevant_cols, mc_processes, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Digression on MC Sampling:</b> Recall from the problem setup above that $P_0$ is a mixture of distributions $P_{0, i}$ each with a ratio $a_i$. These ratios describe the relative frequency of each background process in the mixture. Now, MC sampling doesn't only simulate background/higgs collisions. It also simulates the detection process. Because of that, lots of MC generated events won't be detected. Thus, the number of entries for each mc processes dataframe is only a small fraction of the number of MC events generated for that process. For actual numbers of MC events look at <b>nevt_*</b> in util.py. Moreover, the number of MC events for a given process doesn't necessarily match the <i>expected</i> number of events for that process, which is given by <b>luminosity x cross-section</b> (again, look at util.py). Thus, to accurately represent $P_0, Q$, we have to rescale/re-weigh each of the MC processes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2b) (0.5 pt) Create a list <b>weights</b> containing the weights of each of the MC processes. Use <b>compute_weights()</b> in util.py. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = util.compute_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2c) (0.5 pt) For each of the MC <i>background</i> processes, calculate the sampling frequencies needed to obtain an accurate sample from $P_0$. The sampling frequencies should add up to one. Store these frequencies in a list <b>P0_frequencies</b>. <br>\n",
    "\n",
    "<b>Hint: </b> Your answer should depend on the sizes of the MC background processes in <b>mc_processes</b>. Also, first elemnt of <b>weights</b> describes the weight of the Higgs process and is irrelevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling frequencies for background processes (P0): [0.0005345512091282337, 0.0012453005511374687, 0.0005345408682187206, 0.012448711948797233, 0.7520465210134404, 0.23319037440927792]\n"
     ]
    }
   ],
   "source": [
    "background_weights = weights[1:]\n",
    "total_background_weight = sum(background_weights)\n",
    "P0_frequencies = [weight / total_background_weight for weight in background_weights]\n",
    "print(\"sampling frequencies for background processes (P0):\", P0_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2d) (0.5 pt) Use <b>split</b> in util.py to split the data into three partitions <b>train_processes, sim_processes, synth_processes</b>. <b>train_processes</b> is the training MC data we'll use to learn properties/features about $P_0, Q$ (see below). <b>sim_processes</b> will be used to sample from the background distribution $P_0$ using each of the MC background processes. This will used to perform hypothesis testing. In past years, <b>synth_processes</b> was used for the last phase of the project (which will not be the case this year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processes, sim_processes, synth_processes = util.split(mc_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2e) (0.5 pt) Add a new column <b>weight</b> for each process in <b>train_processes</b>. Use the <b>weights</b> obtained above which correspond one-by-one to the processes in <b>train_processes</b>. These weights will be used to learn about the properties of $Q, P_0$ later-on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(train_processes)):\n",
    "    train_processes[i]['weight'] = weights[i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Fomulating the problem</b>\n",
    "\n",
    "Recall that $P_0, Q$ represent the background/Higgs data distributions, respectively. Whatever the ground truth is, the observed experimental data is a mixture of both. More precisely, it has a distribution $\\pi Q + (1-\\pi) P_0$ for some mixing ratio $\\pi \\in [0, 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3a) (0.5 pt) If our task is to conclude the discovery of the Higgs Boson through hypothesis testing, describe the null hypothesis in words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null: The observed experimental data is solely due to background processes and the higgs doesnt contribute to the data.\n",
    "Alternative: The experimental data has some proportional contribution from the Higgs boson. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3b) (0.5 pt) Describe the null/alternative hypotheses mathematically in terms of $\\pi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H_0: pi = 0 ; H_1: pi !=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>4. Obtaining $\\hat{h}$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the Neyman-Pearson lemma, the uniformily most powerful test uses the statistic \n",
    "\n",
    "$$\n",
    "h^*(x) = \\mathrm{log}\\Big(\\frac{Q(x)}{P_0(x)}\\Big).\n",
    "$$\n",
    "\n",
    "Hence, it is natural to study $h^*$. Note, however, that we don't have a closed-form expression for $P_0(x), Q(x)$; only samples. Thus, we'll use the following result:\n",
    "\n",
    "Any classifier trained with cross-entropy loss\n",
    "should approximate $h^∗(x)$. More precisely, define the log-loss as usual\n",
    "\n",
    "$$\n",
    "l(y, p) = y \\mathrm{log} \\Big(\\frac{1}{p}\\Big) + (1-y) \\mathrm{log}\\Big(\\frac{1}{1-p}\\Big), \\;\\;\\; y\\in \\{0, 1\\}, p\\in [0, 1].\n",
    "$$\n",
    "\n",
    "Now, given $n_0 = \\nu_0 n$ samples with $y_i = 0$ and $X_i \\sim P_0$ and $n_1 = \\nu_1 n$\n",
    "samples with $X_i \\sim Q$ we have as $n = n_0 + n_1 \\rightarrow \\infty$:\n",
    "\n",
    "$$\n",
    "\\mathrm{log}\\Big(\\frac{\\hat{f}_n}{1-\\hat{f}_n}\\Big)\\rightarrow \\mathrm{log}\\Big(\\frac{\\nu_1}{\\nu_0}\\Big) + h^*\n",
    "$$\n",
    "\n",
    "where $\\hat{f}_n$ is the sigmoid output of the classifier.\n",
    "If we have $\\nu_0 = \\nu_1$, then we have $\\mathrm{log}\\Big(\\frac{\\hat{f}_n}{1-\\hat{f}_n}\\Big)$ is approximately $h^*$.\n",
    "\n",
    "To that end, our choice of the classifier is a neural network. Moreover, we'll use the <b>weights</b> computed above to re-weigh the training samples for the reasons described in <b>Digression on MC Sampling</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4a) (0.5 pt) Combine all processes in <b>train_processes</b> into a single dataframe <b>X</b>. Pop the <b>weight</b> column from <b>X</b> and store it in a variable <b>training_weights</b>. Pop the <b>signal</b> column from <b>X</b> into a variable <b>Y</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat(train_processes)\n",
    "training_weights = X.pop('weight')\n",
    "Y = X.pop('signal')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4b) (1 pt) Define a PyTorch <b>Dataset</b> class named <b>MyDataset</b> that indexes the samples <b>X[i], Y[i], training_weights[i]</b>. See https://pytorch.org/tutorials/beginner/basics/data_tutorial.html for tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, H, training_weights):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y.values, dtype=torch.float32)\n",
    "        self.training_weights = torch.tensor(training_weights.values, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx], self.training_weights[idx]\n",
    "    \n",
    "    \n",
    "dataset = MyDataset(X, Y, training_weights)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4c) (1 pt) Define a model <b>MyModel</b> that takes as argument the input dimension and returns a Dense Nueral Network (DNN) with three hidden layers. Each hidden layer has 10 units with ReLU activation. The output layer has a single unit with sigmoid activation. See https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html for tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "        \n",
    "model = MyModel(X.shape[1])\n",
    "# notice that reduction='none'\n",
    "criterion = nn.BCELoss(reduction='none')\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4d) (2 pts) Construct the model, then fit it to <b>X, Y</b> weighted by <b>training_weights</b> for 10 epochs. See https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html for tutorial. Print the loss for each epoch.\n",
    "\n",
    "<b>Note: </b> Make sure the loss is decreasing and after 10 epochs of training, the loss is less than half the loss in the first epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 8.25e-03\n",
      "epoch = 2, loss = 5.88e-03\n",
      "epoch = 3, loss = 5.25e-03\n",
      "epoch = 4, loss = 4.97e-03\n",
      "epoch = 5, loss = 4.75e-03\n",
      "epoch = 6, loss = 4.61e-03\n",
      "epoch = 7, loss = 4.29e-03\n",
      "epoch = 8, loss = 4.51e-03\n",
      "epoch = 9, loss = 4.15e-03\n",
      "epoch = 10, loss = 3.88e-03\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    avg_loss = 0\n",
    "    for x, y, w in dataloader:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x.float())\n",
    "        loss = criterion(outputs.squeeze(), y.float())\n",
    "        loss = (loss * w).mean() \n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        avg_loss += loss.item()\n",
    "    avg_loss /= len(dataloader)\n",
    "    print(f'epoch = {epoch}, loss = {avg_loss:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4e) (2 pts) Plot the histograms of the predicted probabilities of (1) X with label Y=0, (2) X with label Y=1, and (3) <b>data</b> (the experimental data). Show three histograms in the same plot with transparency=0.2. Remember to show the legends as well.\n",
    "\n",
    "<b>Hint:</b> transparency can be set by using the argument <b>alpha</b> in the plt.hist function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (fc1): Linear(in_features=40, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (fc4): Linear(in_features=10, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "def predict_probs(model, dataloader):\n",
    "    probs = []\n",
    "    for inputs, _, _ in dataloader:\n",
    "        outputs = model(inputs.float())\n",
    "        probs.extend(outputs.squeeze().detach().numpy())\n",
    "    return probs\n",
    "    \n",
    "Y_array = np.array(Y)\n",
    "predicted_probs_data = predict_probs(model, dataloader)\n",
    "predicted_probs_Y0 = [predicted_probs_data[i] for i in range(len(predicted_probs_data)) if Y_array[i] == 0]\n",
    "predicted_probs_Y1 = [predicted_probs_data[i] for i in range(len(predicted_probs_data)) if Y_array[i] == 1]\n",
    "\n",
    "plt.hist(predicted_probs_Y0, bins=50, alpha=0.2, label='predicted probabilities (Y=0)', color='blue')\n",
    "plt.hist(predicted_probs_Y1, bins=50, alpha=0.2, label='predicted probabilities (Y=1)', color='red')\n",
    "plt.hist(predicted_probs_data, bins=50, alpha=0.2, label='Experimental Data', color='green')\n",
    "\n",
    "plt.xlabel('predicted probabillities')\n",
    "plt.ylabel('frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(model)\n",
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4f) (0.5 pt) What can you guess from looking at the histograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That although faint, there is a predicted likelihood that we can classify some of the proportion of the data to Higgs Boson signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the discussion above, define\n",
    "\n",
    "$$\n",
    "\\hat{h} := \\mathrm{log}\\Big(\\frac{\\hat{f}_n}{1-\\hat{f}_n}\\Big)\n",
    "$$\n",
    "\n",
    "which will act as an approximation of $h^*$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Evaluating $\\hat{h}$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4g) (1 pt) Argue that $\\hat{h}$ can be obtained from the trained DNN above but with replacing the sigmoid activation with a linear activation (i.e. $f(x) = x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By doing so, we will not be getting probabilities of classification. Rather, we could get a continuous value not necessarily bounded in 0 to 1.\n",
    "So this linear activation would represent the log-odds ratio approximation (logit is inverse of sigmoid) of h*(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4h) (3 pts) Re-write the <b>MyModel</b> class to remove the sigmoid activation. Then, train it on <b>X, Y</b> weighted by <b>training_weights</b> for 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel(X.shape[1])\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "opt = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 4.64e-02\n",
      "epoch = 2, loss = 4.33e-02\n",
      "epoch = 3, loss = 4.27e-02\n",
      "epoch = 4, loss = 4.10e-02\n",
      "epoch = 5, loss = 3.12e-02\n",
      "epoch = 6, loss = 2.62e-02\n",
      "epoch = 7, loss = 2.50e-02\n",
      "epoch = 8, loss = 2.45e-02\n",
      "epoch = 9, loss = 2.41e-02\n",
      "epoch = 10, loss = 2.37e-02\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 11):\n",
    "    avg_loss = 0\n",
    "    for x, y, w in dataloader:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x.float())\n",
    "        loss = criterion(outputs.squeeze(), y.float())\n",
    "        loss = torch.mean(loss * w.view(-1, 1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        avg_loss += loss.item()\n",
    "    avg_loss /= len(dataloader)\n",
    "    print(f'epoch = {epoch}, loss = {avg_loss:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4i) (0.5 pt) Construct a $P_0$ sample of size $10^5$ using the MC background processes and <b>P0_frequencies</b> computed above. Make sure to pop <b>signal</b> columns at the end. <br>\n",
    "<b>Note: </b> the first entry in <b>sim_processes</b> concerns Higgs and is irrelevant here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_processes_bg = sim_processes[1:]\n",
    "P0_sample = util.sample(sim_processes_bg, P0_frequencies, 10**5)\n",
    "P0_sample.drop(columns='signal', inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4j) (0.5 pt) Compute $\\hat{h}(x)$ for every $x$ in the $P_0$ sample. Store the result in a numpy array <b>h_sim</b>. Compute $\\hat{h}(x)$ for every $x$ in experimental data. Store the result in a numpy array <b>h_data</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h_hat(model, x):\n",
    "    output = model(torch.tensor(x, dtype=torch.float32))\n",
    "    \n",
    "    output = output.detach().numpy()\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    h_hat = np.log(output / (1 - output))\n",
    "    \n",
    "    return h_hat\n",
    "\n",
    "h_sim = []\n",
    "for index, row in P0_sample.iterrows():\n",
    "    x = row.values\n",
    "    h_hat = compute_h_hat(model, x)\n",
    "    h_sim.append(h_hat)\n",
    "\n",
    "h_sim = np.array(h_sim)\n",
    "\n",
    "h_data = []\n",
    "for index, row in data.iterrows():\n",
    "    x = row.values\n",
    "    h_hat = compute_h_hat(model, x)\n",
    "    h_data.append(h_hat)\n",
    "\n",
    "h_data = np.array(h_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>5. Statistical Tests</b> \n",
    "\n",
    "Instead of just looking at the plot, we want to proof the existence of Higgs boson by using hypothesis testing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test 1: CLT Test:</b> The CLT test uses the fact that $\\hat{h}(X_i)$ are i.i.d where $X_i$ comes from experimental data. We then assume that $n$ is large enough such that we can use the central limit theorem. \n",
    "\n",
    "However, how large of an $n$ we need depends on how close the distribution of $\\hat{h}(X_i)$ is to a gaussian to start with. The closer to a Gaussian, the smaller the $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5a) (0.5 pt) Compute and print the mean and standard deviation of $\\hat{h}$ on the $P_0$ sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean for h-sim: -16.115984\n",
      "standard deviation for h-sim: 0.22377415\n"
     ]
    }
   ],
   "source": [
    "mean_h_sim, dev_h_sim = np.mean(h_sim), np.std(h_sim)\n",
    "print(\"mean for h-sim:\", mean_h_sim)\n",
    "print('standard deviation for h-sim:', dev_h_sim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5b) (0.5 pt) Calculate the Kolmogorov–Smirnov test for goodness of fit of $\\hat{h}(X_i)$ under $P_0$ against a gaussian with mean/std being the mean/std of $\\hat{h}$ under $P_0$, respectively. Print the p-value.\n",
    "\n",
    "<b>Hint:</b> feel free to use scipy.stats.norm and scipy.stats.kstest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ks test p-val: [0.]\n"
     ]
    }
   ],
   "source": [
    "gaussian_dist = norm(loc=mean_h_sim, scale=dev_h_sim)\n",
    "ks_statistic, p_value = kstest(h_sim, gaussian_dist.cdf)\n",
    "print(\"Ks test p-val:\", p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5c) (1 pt) Based on the test result, argue against proceeding with the CLT test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the extremely low pvalue, the goodness of fit test to the gaussian under simulated parameters should reject the idea that \n",
    "the simulated is a normal distribution.So the h-hat is not very likely to be close to gaussian, so the CLT could likely give unreliable results with limited n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test 2: Wilcoxon Sum-Rank Test:</b> We'll be using the Mann–Whitney $U$ test (also denoted by the Wilcoxon Sum-Rank test). You can learn more about it here: https://en.wikipedia.org/wiki/Mann%E2%80%93Whitney_U_test. It's a non-parametric test that tries to reject the null hypothesis that the distributions of two populations $X, Y$ are identical for the alternative that one population is stochastically greater than the other. More specifically, given two independent samples $(X_1, \\dots, X_n), (Y_1, \\dots Y_m)$ it computes the following $U$ statistics\n",
    "$$\n",
    "U := \\sum_{i=1}^n\\sum_{j=1}^m 1_{X_i > Y_j}.\n",
    "$$\n",
    "Here, we assume $X_i$ and $Y_j$ are continuous random variables so $\\mathbb{P}(X_i = Y_j) = 0, \\forall i, j$.\n",
    "For sample sizes $n, m$ small, exact distribution of $U$ under the null hypothesis is known and tabulated. For large values of $n, m$ a Gaussian approximation is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5d) (1 pt) Argue that we can use a one-sided alternative hypothesis when implementing the Wilcoxon test on $\\hat{h}(X_i)$ under experimental data vs. $P_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use a one-sided alternative hypotheses that we can focus on to find evidence that the experimental data\n",
    "has a tendency to yield larger values of h hatcompared to that from P_0 alone. This essentially attibutes a test for measurable\n",
    "significance between the P0sample(no higgs) and the data(potentially higgs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wilcoxon test in scipy.stats.mannwhitneyu used a Gaussian approximation to compute the p-value, which is justified as we have large values of $n, m$. Let's now explore this approximation, and try to implement the test manually ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5e) (1 pt) Compute $\\mathbb{E}[U]$.1}{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "double sum of P(X_i > Y_i). Due to Stochastic ordering, each observation is equally likely to be greater.\n",
    "So E[U] = n*m/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5f) (2 pts) Compute $\\mathbb{E}[U^2]$<br>\n",
    "<b>Hint:</b> Expand the square then simply/discuss by cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U^2 == double sum of (Indicator for ij) * double sum of (Indicator for kl)\n",
    "For i!=k and j != l There are n*m terms for both, so (n*m)^2\n",
    "for i = k and j !=l n for i and k, and m*m for j and l, so n*m^2\n",
    "wlog for i != k and j = l, so n^2 *m\n",
    "and i = k with j=l, we get k*l .\n",
    "\n",
    "There are three non i!=k and j != l cases, along knowing that (Indicator for ij) and (Indicator for kl) are independent and product of expectation is == 1/4\n",
    "\n",
    "So E(U^2) = (1/3*1/4*((n*m^2) + (n^2*m) + n*m)) + 1/4*(n*m)^2\n",
    "which is \n",
    "\n",
    "=1/12*(n*m(1 +n +m)+ 3*(n*m)^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5g) (2 pts) Compute $\\mathrm{Var}(U)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Var(U)= 1/12*(n*m(1 +n +m))\n",
    "\n",
    "because E(U^2) - E(U)^2 is \n",
    "\n",
    "(1/3* 1/4*((n*m^2) + (n^2*m) + n*m)) + 1/4*(n*m)^2 - (n*m/2)^2\n",
    "\n",
    "which is\n",
    "\n",
    "=1/12*(n*m(1 +n +m))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5h) (1 pt) We cannot directly use the Central Limit Theorem to approximate the distribution of $U$ by Gaussian. Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Mann-Whitney U statistic is not directly based on the sum of i.i.d random varaibles, as verified by our previous computations from stochastically ranked variables. \n",
    "The distributoin of the statistic depends on the distribution of the compared samples.\n",
    "So the central limit is not satisfied from the Whitney test to approximate U by the Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it is not so simple to conclude that the Gaussian approximation is valid. However, if you're interested, you can read https://www.tandfonline.com/doi/abs/10.1080/10691898.2010.11889486. In this paper, the authors recommend the approximation by a Gaussian with the mean/variance you computed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5i) (2 pts) Assuming the Gaussian approximation, write a function <b>wilcoxon_test</b> that takes in the two arrays <b>h1, h2</b> and output the $U$-statistic and the $p$-value. Print the $U$-statistic and the $p$-value.\n",
    "\n",
    "<b>Hint</b>: going over all pairs accross <b>h_data, h_sim</b> is inefficient. For a more efficient implementation, start by sorting the combined values of <b>h_data, h_sim</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U = 495\n",
      "p-value = 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "def wilcoxon_test(h1, h2):\n",
    "    combined_values = np.concatenate([h1, h2])\n",
    "    combined_values_sorted = np.sort(combined_values)\n",
    "    ranks = np.argsort(combined_values_sorted) + 1\n",
    "    \n",
    "    n = len(h1)\n",
    "    m = len(h2)\n",
    "    \n",
    "    ranks_h1 = ranks[:len(h1)]\n",
    "    ranks_h2 = ranks[len(h1):]\n",
    "    U = np.sum(ranks_h1 * (ranks_h1 <= len(h1)))\n",
    "    \n",
    "    mean_U =(n * m)/ 2\n",
    "    var_U= (n * m *(1 + m + n))/ 12\n",
    "    \n",
    "    z = (U -mean_U) / np.sqrt(var_U)\n",
    "    pvalue = 2 * (1 - norm.cdf(np.abs(z)))\n",
    "    \n",
    "    return U, pvalue\n",
    "    \n",
    "U, pvalue = wilcoxon_test(h_data, h_sim)\n",
    "print(f'U = {U}')\n",
    "print(f'p-value = {pvalue:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Test 3: Classification Accuracy Test:</b> Previosly, we compare the distributions of <b>h_sim</b> and <b>h_data</b>, which are both logit outputs of the model. \n",
    "What if we instead use the class predictions.\n",
    "Essentially, we compare the ratio of 0's and 1's predictions based on the classifier for both distributions.\n",
    "Intuitively, it seems to be a worse test than the ones based on logits because we loss some informations when we threshold the outputs into 0's and 1's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5j) (0.5 pt) Get the 0-1 predictions of <b>h_sim</b> and <b>h_data</b> by the model by thresholding the logits at 0. Name the arrays as <b>y_sim</b> and <b>y_data</b>, respectively. Then, print the percentage of 1's in each distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1's in h_sim: 0.01%\n",
      "Percentage of 1's in h_data: 1.62%\n"
     ]
    }
   ],
   "source": [
    "y_sim = []\n",
    "y_data = []\n",
    "for value in h_sim:\n",
    "    if value > 0:\n",
    "        y_sim.append(1)\n",
    "    else:\n",
    "        y_sim.append(0)\n",
    "\n",
    "for value in h_data:\n",
    "    if value > 0:\n",
    "        y_data.append(1)\n",
    "    else:\n",
    "        y_data.append(0)\n",
    "\n",
    "count_ones_sim = sum(y_sim)\n",
    "count_ones_data = sum(y_data)\n",
    "total_sim = len(y_sim)\n",
    "total_data = len(y_data)\n",
    "percentage_ones_sim = (count_ones_sim / total_sim) * 100\n",
    "percentage_ones_data = (count_ones_data / total_data) * 100\n",
    "\n",
    "print(f'Percentage of 1\\'s in h_sim: {percentage_ones_sim:.2f}%')\n",
    "print(f'Percentage of 1\\'s in h_data: {percentage_ones_data:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5k) (0.5 pt) What do the percentages of 1's tell you? Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That there is a higher lproportion of class 1 (presence of Higgs) in the experimental data than in the simulated. It makes sense given that the simulated is composed of background distribution sampling, while the experimental data could potentially have this higgs presence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5l) (2 pts) Now, perform the <b>classification accuracy test</b> by assuming both distributions are binomial that can be approximated as a Gaussian distribution. Write a function <b>accuracy_test</b> which takes in two arrays <b>y1, y2</b> and output the $p$-value. Print the $p$-value.\n",
    "\n",
    "<b>Hint</b>: similar to pooled two-sampled t-test, but the variance is based on Binomial distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 0.00e+00\n"
     ]
    }
   ],
   "source": [
    "def accuracy_test(y1, y2):\n",
    "    p1 = np.mean(y1)\n",
    "    p2 = np.mean(y2)\n",
    "    \n",
    "    p_pooled = (np.sum(y1) + np.sum(y2)) / (len(y1) + len(y2))\n",
    "    se_pooled = np.sqrt(p_pooled * (1 - p_pooled) * (1 / len(y1) + 1 / len(y2)))\n",
    "    \n",
    "    z = (p1 - p2) / se_pooled\n",
    "    pvalue = 2 * (1 - norm.cdf(np.abs(z)))\n",
    "    \n",
    "    return pvalue\n",
    "    \n",
    "pvalue = accuracy_test(y_data, y_sim)\n",
    "print(f'p-value = {pvalue:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>6. Enhanced Features</b> Recall that we discarded a bunch of features as part of the data trimming process. Three of these features were <b>mass, mass_z1, mass_z2</b>. You can think of these features as \"smart features\" that are engineered/computed through the use of physics knowledge by smart humans to help differentiate Higgs from background. We are now going to explore the effects of using these features in addition to the ones we used previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6a) (26 pts) Repeat the process and calculate the $p$-values of the Wilcoxon test and the accuracy test without discarding the \"smart features\" <b>mass, mass_z1, mass_z2</b>. Print the $p$-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 1, loss = 4.51e-02\n",
      "epoch = 2, loss = 2.98e-02\n",
      "epoch = 3, loss = 1.50e-02\n",
      "epoch = 4, loss = 1.40e-02\n",
      "epoch = 5, loss = 1.35e-02\n",
      "epoch = 6, loss = 1.34e-02\n",
      "epoch = 7, loss = 1.32e-02\n",
      "epoch = 8, loss = 1.31e-02\n",
      "epoch = 9, loss = 1.28e-02\n",
      "epoch = 10, loss = 1.28e-02\n",
      "U = 495\n",
      "p-value_wilx = 0.0000000e+00\n",
      "p-value_wilx is physics significant\n",
      "p-value = 0.0000000e+00\n",
      "p-value_accu is physics significant\n"
     ]
    }
   ],
   "source": [
    "data = util.load_expr_data()\n",
    "mc_processes = util.load_processes()\n",
    "\n",
    "object_cols = ['PID1', 'PID2', 'PID3', 'PID4'] \n",
    "\n",
    "# Vanilla\n",
    "irrelevant_cols = ['Unnamed: 0', 'Run', 'Event', 'Q1', 'Q2', 'Q3', 'Q4']\n",
    "\n",
    "# your code here\n",
    "OH_encoder = util.encoder(mc_processes, object_cols)\n",
    "data, mc_processes = util.trim(OH_encoder, object_cols, irrelevant_cols, mc_processes, data)\n",
    "\n",
    "weights = util.compute_weights()\n",
    "background_weights = weights[1:]\n",
    "total_background_weight = sum(background_weights)\n",
    "P0_frequencies = [weight / total_background_weight for weight in background_weights]\n",
    "\n",
    "train_processes, sim_processes, synth_processes = util.split(mc_processes)\n",
    "\n",
    "for i in range(len(train_processes)):\n",
    "    train_processes[i]['weight'] = weights[i] \n",
    "\n",
    "X = pd.concat(train_processes)\n",
    "training_weights = X.pop('weight')\n",
    "Y = X.pop('signal')\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, H, training_weights):\n",
    "        self.X = torch.tensor(X.values, dtype=torch.float32)\n",
    "        self.Y = torch.tensor(Y.values, dtype=torch.float32)\n",
    "        self.training_weights = torch.tensor(training_weights.values, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx], self.training_weights[idx]\n",
    "    \n",
    "    \n",
    "dataset = MyDataset(X, Y, training_weights)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 10)\n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 10)\n",
    "        self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = MyModel(X.shape[1])\n",
    "criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    avg_loss = 0\n",
    "    for x, y, w in dataloader:\n",
    "        opt.zero_grad()\n",
    "        outputs = model(x.float())\n",
    "        loss = criterion(outputs.squeeze(), y.float())\n",
    "        loss = torch.mean(loss * w.view(-1, 1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        avg_loss += loss.item()\n",
    "    avg_loss /= len(dataloader)\n",
    "    print(f'epoch = {epoch}, loss = {avg_loss:.2e}')\n",
    "\n",
    "sim_processes_bg = sim_processes[1:]\n",
    "P0_sample = util.sample(sim_processes_bg, P0_frequencies, 10**5)\n",
    "P0_sample.drop(columns='signal', inplace=True)\n",
    "\n",
    "def compute_h_hat(model, x):\n",
    "    output = model(torch.tensor(x, dtype=torch.float32))\n",
    "    \n",
    "    output = output.detach().numpy()\n",
    "    output = np.clip(output, 1e-7, 1 - 1e-7)\n",
    "\n",
    "    h_hat = np.log(output / (1 - output))\n",
    "    \n",
    "    return h_hat\n",
    "\n",
    "h_sim = []\n",
    "for index, row in P0_sample.iterrows():\n",
    "    x = row.values\n",
    "    h_hat = compute_h_hat(model, x)\n",
    "    h_sim.append(h_hat)\n",
    "\n",
    "h_sim = np.array(h_sim)\n",
    "\n",
    "h_data = []\n",
    "for index, row in data.iterrows():\n",
    "    x = row.values\n",
    "    h_hat = compute_h_hat(model, x)\n",
    "    h_data.append(h_hat)\n",
    "\n",
    "h_data = np.array(h_data)\n",
    "\n",
    "U, pvalue = wilcoxon_test(h_data, h_sim)\n",
    "print(f'U = {U}')\n",
    "print(f'p-value_wilx = {pvalue:.7e}')\n",
    "\n",
    "#five sigma check to p\n",
    "physics_significance = 2* norm.cdf(-5)\n",
    "if p_value <= physics_significance:\n",
    "    print(f'p-value_wilx is physics significant')\n",
    "\n",
    "\n",
    "y_sim = []\n",
    "y_data = []\n",
    "\n",
    "for value in h_sim:\n",
    "    if value > 0:\n",
    "        y_sim.append(1)\n",
    "    else:\n",
    "        y_sim.append(0)\n",
    "\n",
    "for value in h_data:\n",
    "    if value > 0:\n",
    "        y_data.append(1)\n",
    "    else:\n",
    "        y_data.append(0)\n",
    "\n",
    "pvalue = accuracy_test(y_data, y_sim)\n",
    "print(f'p-value = {pvalue:.7e}')\n",
    "\n",
    "#five sigma check to p\n",
    "physics_significance = 2* norm.cdf(-5)\n",
    "if p_value <= physics_significance:\n",
    "    print(f'p-value_accu is physics significant')\n",
    "\n",
    "h1 = np.array([1.2, 3.4, 5.6, 7.8])\n",
    "h2 = np.array([2.1, 4.3, 6.5, 8.7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6b) (0.5 pt) Typically, the standard for declaring \"new particle\" is 5 sigmas. Here, with the help of \"smart features,\" have we achieve such standard?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, we do achieve this standard as seen in the prints from the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of 1's in h_sim: 0.00%\n",
      "Percentage of 1's in h_data: 2.83%\n"
     ]
    }
   ],
   "source": [
    "count_ones_sim = sum(y_sim)\n",
    "count_ones_data = sum(y_data)\n",
    "total_sim = len(y_sim)\n",
    "total_data = len(y_data)\n",
    "percentage_ones_sim = (count_ones_sim / total_sim) * 100\n",
    "percentage_ones_data = (count_ones_data / total_data) * 100\n",
    "\n",
    "# Print the percentages\n",
    "print(f'Percentage of 1\\'s in h_sim: {percentage_ones_sim:.2f}%')\n",
    "print(f'Percentage of 1\\'s in h_data: {percentage_ones_data:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, note that this project is a simplied and scaled-down version of the original experiment. We only have 1 Higgs process and 6 background processes whereas the real-world one has 100 Higgs processes and 1000 background processes. Thus, it is much difficult to achieve 5 sigmas in that case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>7. Discussion</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be wrong to select what hypothesis test to use <i>after</i> we look at the test results. Ideally, we should be able to choose hypothesis tests before we look at the data given that the assumptions supporting the test hold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7a) (1 pt) Ignoring the test performance, what assumptions does the CLT test rely on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It assumes independent obsevations, large sample size, and finite variance. It depends on the asymptotic goodness fit to a normal by these conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7b) (1 pt) Why didn't we proceed using the CLT test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the distribtution of the simulated data significantly faltered from a gaussian distribution, as seen when perfoming the KS test for goodness of fit. Due do this dis-normality, the CLT was going to be an unreliable way to make inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7c) (1 pt) Ignoring the test performance, what assumptions does the Wilcoxon test and the accuracy test rely on? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wilcoxon relies on sorted to define ranks, so it is measured ordinally. It doens't directly use CLT, but it does use Gaussian approximation. The Accuracy test here relies on assumption of binomials approximated by Gaussians because we look at 1-0 binary prediction comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " (7d) (1 pt) Are you confident in using the Wilcoxon test or the accuracy test?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am confident in using the accuracy test. It seeems to more significantly compare proportions of 1-classified events,\n",
    "rather than relying on ordinality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7e) (1 pt) Using the results of the test you selected above, did the \"smart features\" enhance the detection capability? Why do you think so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is still significant in both cases. However, adding those features seemed like overkill considering the p-values were already so low. So I think these extra features likely helped the model considering that we have detected a clearer: Percentage of 1's in h_sim: 0.00%\r\n",
    "Percentage of 1's in h_data:2.83%%.........given the new features included. The p-values were already so small however. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(7f) (1 pt) Comparing the results of the Wilcoxon test and the accuracy test. What do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That both test had significantly high significance given the low p-values. It seems to be fairly clear that the higgs boson contributes to the decay, so we should reject the null. However, more tangential results should be seen from the accuracy test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, proven in some papers, although thresholding to 0's and 1's, the accuracy test can achieve the same optimal  detection capability compared to other tests that use the logits (or the probabilities).\n",
    "\n",
    "Congrats on finishing Discovery of Higgs Boson!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
